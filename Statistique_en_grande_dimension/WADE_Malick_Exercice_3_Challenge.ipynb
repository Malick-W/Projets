{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities and Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WADE El Hadji Malick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt    \n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib # save and load models\n",
    "\n",
    "# # save the model to disk\n",
    "# filename = 'modeles/SVM'\n",
    "# joblib.dump(SVM, filename)\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "path_Home = \"/home/jovyan/work/Projets/Projets/Statistique_en_grande_dimension/donnees_challenge\"\n",
    "#path = \"/users/mmath/wade/Bureau/Data/Statistique_en_grande_dimension\"\n",
    "\n",
    "Xtrainchallenge = pd.read_csv(path_Home + \"/Xtrainchallenge.txt\",  sep=' ')\n",
    "Ytrainchallenge = pd.read_csv(path_Home + \"/Ytrainchallenge.txt\",  sep=' ')\n",
    "\n",
    "Xtestchallenge = pd.read_csv(path_Home + \"/Xtestchallenge.txt\",  sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes:  1000\n",
      "Colonnes:  500\n",
      "---------------\n",
      "\n",
      "Variables:\n",
      "V1      int64\n",
      "V2      int64\n",
      "V3      int64\n",
      "V4      int64\n",
      "V5      int64\n",
      "V6      int64\n",
      "V7      int64\n",
      "V8      int64\n",
      "V9      int64\n",
      "V10     int64\n",
      "V11     int64\n",
      "V12     int64\n",
      "V13     int64\n",
      "V14     int64\n",
      "V15     int64\n",
      "V16     int64\n",
      "V17     int64\n",
      "V18     int64\n",
      "V19     int64\n",
      "V20     int64\n",
      "V21     int64\n",
      "V22     int64\n",
      "V23     int64\n",
      "V24     int64\n",
      "V25     int64\n",
      "V26     int64\n",
      "V27     int64\n",
      "V28     int64\n",
      "V29     int64\n",
      "V30     int64\n",
      "V31     int64\n",
      "V32     int64\n",
      "V33     int64\n",
      "V34     int64\n",
      "V35     int64\n",
      "V36     int64\n",
      "V37     int64\n",
      "V38     int64\n",
      "V39     int64\n",
      "V40     int64\n",
      "V41     int64\n",
      "V42     int64\n",
      "V43     int64\n",
      "V44     int64\n",
      "V45     int64\n",
      "V46     int64\n",
      "V47     int64\n",
      "V48     int64\n",
      "V49     int64\n",
      "V50     int64\n",
      "V51     int64\n",
      "V52     int64\n",
      "V53     int64\n",
      "V54     int64\n",
      "V55     int64\n",
      "V56     int64\n",
      "V57     int64\n",
      "V58     int64\n",
      "V59     int64\n",
      "V60     int64\n",
      "V61     int64\n",
      "V62     int64\n",
      "V63     int64\n",
      "V64     int64\n",
      "V65     int64\n",
      "V66     int64\n",
      "V67     int64\n",
      "V68     int64\n",
      "V69     int64\n",
      "V70     int64\n",
      "V71     int64\n",
      "V72     int64\n",
      "V73     int64\n",
      "V74     int64\n",
      "V75     int64\n",
      "V76     int64\n",
      "V77     int64\n",
      "V78     int64\n",
      "V79     int64\n",
      "V80     int64\n",
      "V81     int64\n",
      "V82     int64\n",
      "V83     int64\n",
      "V84     int64\n",
      "V85     int64\n",
      "V86     int64\n",
      "V87     int64\n",
      "V88     int64\n",
      "V89     int64\n",
      "V90     int64\n",
      "V91     int64\n",
      "V92     int64\n",
      "V93     int64\n",
      "V94     int64\n",
      "V95     int64\n",
      "V96     int64\n",
      "V97     int64\n",
      "V98     int64\n",
      "V99     int64\n",
      "V100    int64\n",
      "V101    int64\n",
      "V102    int64\n",
      "V103    int64\n",
      "V104    int64\n",
      "V105    int64\n",
      "V106    int64\n",
      "V107    int64\n",
      "V108    int64\n",
      "V109    int64\n",
      "V110    int64\n",
      "V111    int64\n",
      "V112    int64\n",
      "V113    int64\n",
      "V114    int64\n",
      "V115    int64\n",
      "V116    int64\n",
      "V117    int64\n",
      "V118    int64\n",
      "V119    int64\n",
      "V120    int64\n",
      "V121    int64\n",
      "V122    int64\n",
      "V123    int64\n",
      "V124    int64\n",
      "V125    int64\n",
      "V126    int64\n",
      "V127    int64\n",
      "V128    int64\n",
      "V129    int64\n",
      "V130    int64\n",
      "V131    int64\n",
      "V132    int64\n",
      "V133    int64\n",
      "V134    int64\n",
      "V135    int64\n",
      "V136    int64\n",
      "V137    int64\n",
      "V138    int64\n",
      "V139    int64\n",
      "V140    int64\n",
      "V141    int64\n",
      "V142    int64\n",
      "V143    int64\n",
      "V144    int64\n",
      "V145    int64\n",
      "V146    int64\n",
      "V147    int64\n",
      "V148    int64\n",
      "V149    int64\n",
      "V150    int64\n",
      "V151    int64\n",
      "V152    int64\n",
      "V153    int64\n",
      "V154    int64\n",
      "V155    int64\n",
      "V156    int64\n",
      "V157    int64\n",
      "V158    int64\n",
      "V159    int64\n",
      "V160    int64\n",
      "V161    int64\n",
      "V162    int64\n",
      "V163    int64\n",
      "V164    int64\n",
      "V165    int64\n",
      "V166    int64\n",
      "V167    int64\n",
      "V168    int64\n",
      "V169    int64\n",
      "V170    int64\n",
      "V171    int64\n",
      "V172    int64\n",
      "V173    int64\n",
      "V174    int64\n",
      "V175    int64\n",
      "V176    int64\n",
      "V177    int64\n",
      "V178    int64\n",
      "V179    int64\n",
      "V180    int64\n",
      "V181    int64\n",
      "V182    int64\n",
      "V183    int64\n",
      "V184    int64\n",
      "V185    int64\n",
      "V186    int64\n",
      "V187    int64\n",
      "V188    int64\n",
      "V189    int64\n",
      "V190    int64\n",
      "V191    int64\n",
      "V192    int64\n",
      "V193    int64\n",
      "V194    int64\n",
      "V195    int64\n",
      "V196    int64\n",
      "V197    int64\n",
      "V198    int64\n",
      "V199    int64\n",
      "V200    int64\n",
      "V201    int64\n",
      "V202    int64\n",
      "V203    int64\n",
      "V204    int64\n",
      "V205    int64\n",
      "V206    int64\n",
      "V207    int64\n",
      "V208    int64\n",
      "V209    int64\n",
      "V210    int64\n",
      "V211    int64\n",
      "V212    int64\n",
      "V213    int64\n",
      "V214    int64\n",
      "V215    int64\n",
      "V216    int64\n",
      "V217    int64\n",
      "V218    int64\n",
      "V219    int64\n",
      "V220    int64\n",
      "V221    int64\n",
      "V222    int64\n",
      "V223    int64\n",
      "V224    int64\n",
      "V225    int64\n",
      "V226    int64\n",
      "V227    int64\n",
      "V228    int64\n",
      "V229    int64\n",
      "V230    int64\n",
      "V231    int64\n",
      "V232    int64\n",
      "V233    int64\n",
      "V234    int64\n",
      "V235    int64\n",
      "V236    int64\n",
      "V237    int64\n",
      "V238    int64\n",
      "V239    int64\n",
      "V240    int64\n",
      "V241    int64\n",
      "V242    int64\n",
      "V243    int64\n",
      "V244    int64\n",
      "V245    int64\n",
      "V246    int64\n",
      "V247    int64\n",
      "V248    int64\n",
      "V249    int64\n",
      "V250    int64\n",
      "V251    int64\n",
      "V252    int64\n",
      "V253    int64\n",
      "V254    int64\n",
      "V255    int64\n",
      "V256    int64\n",
      "V257    int64\n",
      "V258    int64\n",
      "V259    int64\n",
      "V260    int64\n",
      "V261    int64\n",
      "V262    int64\n",
      "V263    int64\n",
      "V264    int64\n",
      "V265    int64\n",
      "V266    int64\n",
      "V267    int64\n",
      "V268    int64\n",
      "V269    int64\n",
      "V270    int64\n",
      "V271    int64\n",
      "V272    int64\n",
      "V273    int64\n",
      "V274    int64\n",
      "V275    int64\n",
      "V276    int64\n",
      "V277    int64\n",
      "V278    int64\n",
      "V279    int64\n",
      "V280    int64\n",
      "V281    int64\n",
      "V282    int64\n",
      "V283    int64\n",
      "V284    int64\n",
      "V285    int64\n",
      "V286    int64\n",
      "V287    int64\n",
      "V288    int64\n",
      "V289    int64\n",
      "V290    int64\n",
      "V291    int64\n",
      "V292    int64\n",
      "V293    int64\n",
      "V294    int64\n",
      "V295    int64\n",
      "V296    int64\n",
      "V297    int64\n",
      "V298    int64\n",
      "V299    int64\n",
      "V300    int64\n",
      "V301    int64\n",
      "V302    int64\n",
      "V303    int64\n",
      "V304    int64\n",
      "V305    int64\n",
      "V306    int64\n",
      "V307    int64\n",
      "V308    int64\n",
      "V309    int64\n",
      "V310    int64\n",
      "V311    int64\n",
      "V312    int64\n",
      "V313    int64\n",
      "V314    int64\n",
      "V315    int64\n",
      "V316    int64\n",
      "V317    int64\n",
      "V318    int64\n",
      "V319    int64\n",
      "V320    int64\n",
      "V321    int64\n",
      "V322    int64\n",
      "V323    int64\n",
      "V324    int64\n",
      "V325    int64\n",
      "V326    int64\n",
      "V327    int64\n",
      "V328    int64\n",
      "V329    int64\n",
      "V330    int64\n",
      "V331    int64\n",
      "V332    int64\n",
      "V333    int64\n",
      "V334    int64\n",
      "V335    int64\n",
      "V336    int64\n",
      "V337    int64\n",
      "V338    int64\n",
      "V339    int64\n",
      "V340    int64\n",
      "V341    int64\n",
      "V342    int64\n",
      "V343    int64\n",
      "V344    int64\n",
      "V345    int64\n",
      "V346    int64\n",
      "V347    int64\n",
      "V348    int64\n",
      "V349    int64\n",
      "V350    int64\n",
      "V351    int64\n",
      "V352    int64\n",
      "V353    int64\n",
      "V354    int64\n",
      "V355    int64\n",
      "V356    int64\n",
      "V357    int64\n",
      "V358    int64\n",
      "V359    int64\n",
      "V360    int64\n",
      "V361    int64\n",
      "V362    int64\n",
      "V363    int64\n",
      "V364    int64\n",
      "V365    int64\n",
      "V366    int64\n",
      "V367    int64\n",
      "V368    int64\n",
      "V369    int64\n",
      "V370    int64\n",
      "V371    int64\n",
      "V372    int64\n",
      "V373    int64\n",
      "V374    int64\n",
      "V375    int64\n",
      "V376    int64\n",
      "V377    int64\n",
      "V378    int64\n",
      "V379    int64\n",
      "V380    int64\n",
      "V381    int64\n",
      "V382    int64\n",
      "V383    int64\n",
      "V384    int64\n",
      "V385    int64\n",
      "V386    int64\n",
      "V387    int64\n",
      "V388    int64\n",
      "V389    int64\n",
      "V390    int64\n",
      "V391    int64\n",
      "V392    int64\n",
      "V393    int64\n",
      "V394    int64\n",
      "V395    int64\n",
      "V396    int64\n",
      "V397    int64\n",
      "V398    int64\n",
      "V399    int64\n",
      "V400    int64\n",
      "V401    int64\n",
      "V402    int64\n",
      "V403    int64\n",
      "V404    int64\n",
      "V405    int64\n",
      "V406    int64\n",
      "V407    int64\n",
      "V408    int64\n",
      "V409    int64\n",
      "V410    int64\n",
      "V411    int64\n",
      "V412    int64\n",
      "V413    int64\n",
      "V414    int64\n",
      "V415    int64\n",
      "V416    int64\n",
      "V417    int64\n",
      "V418    int64\n",
      "V419    int64\n",
      "V420    int64\n",
      "V421    int64\n",
      "V422    int64\n",
      "V423    int64\n",
      "V424    int64\n",
      "V425    int64\n",
      "V426    int64\n",
      "V427    int64\n",
      "V428    int64\n",
      "V429    int64\n",
      "V430    int64\n",
      "V431    int64\n",
      "V432    int64\n",
      "V433    int64\n",
      "V434    int64\n",
      "V435    int64\n",
      "V436    int64\n",
      "V437    int64\n",
      "V438    int64\n",
      "V439    int64\n",
      "V440    int64\n",
      "V441    int64\n",
      "V442    int64\n",
      "V443    int64\n",
      "V444    int64\n",
      "V445    int64\n",
      "V446    int64\n",
      "V447    int64\n",
      "V448    int64\n",
      "V449    int64\n",
      "V450    int64\n",
      "V451    int64\n",
      "V452    int64\n",
      "V453    int64\n",
      "V454    int64\n",
      "V455    int64\n",
      "V456    int64\n",
      "V457    int64\n",
      "V458    int64\n",
      "V459    int64\n",
      "V460    int64\n",
      "V461    int64\n",
      "V462    int64\n",
      "V463    int64\n",
      "V464    int64\n",
      "V465    int64\n",
      "V466    int64\n",
      "V467    int64\n",
      "V468    int64\n",
      "V469    int64\n",
      "V470    int64\n",
      "V471    int64\n",
      "V472    int64\n",
      "V473    int64\n",
      "V474    int64\n",
      "V475    int64\n",
      "V476    int64\n",
      "V477    int64\n",
      "V478    int64\n",
      "V479    int64\n",
      "V480    int64\n",
      "V481    int64\n",
      "V482    int64\n",
      "V483    int64\n",
      "V484    int64\n",
      "V485    int64\n",
      "V486    int64\n",
      "V487    int64\n",
      "V488    int64\n",
      "V489    int64\n",
      "V490    int64\n",
      "V491    int64\n",
      "V492    int64\n",
      "V493    int64\n",
      "V494    int64\n",
      "V495    int64\n",
      "V496    int64\n",
      "V497    int64\n",
      "V498    int64\n",
      "V499    int64\n",
      "V500    int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (\"Lignes: \" ,Xtrainchallenge.shape[0])\n",
    "print (\"Colonnes: \" ,Xtrainchallenge.shape[1])\n",
    "\n",
    "print(\"---------------\")\n",
    "print (\"\\nVariables:\")\n",
    "print(Xtrainchallenge.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Col_train = Xtrainchallenge.std() > 1.5\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Xtrainchallenge = Xtrainchallenge[Col_train]\n",
    "# Xtestchallenge = Xtestchallenge[Col_train]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.3)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrainchallenge = Xtrainchallenge[Col_train]\n",
    "# Xtestchallenge = Xtestchallenge[Col_train]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.3)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On regarde la matrice de corrélation\n",
    "# fig = plt.figure(1, figsize=(50, 50))\n",
    "\n",
    "# # sns.heatmap(round(Xtrainchallenge.corr(),2), cmap=sns.diverging_palette(20, 220, n=200), fmt=\".2f\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Lignes: \" ,Xtrainchallenge.shape[0])\n",
    "# print (\"Colonnes: \" ,Xtrainchallenge.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modèles de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.3)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = Xtrainchallenge\n",
    "# X_test = Xtestchallenge\n",
    "# y_train = Ytrainchallenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components = 150\n",
    "\n",
    "# # TODO: Create an instance of PCA, initializing with n_components=n_components and whiten=True\n",
    "# pca = PCA(n_components=n_components, whiten=True, svd_solver='randomized')\n",
    "\n",
    "# #TODO: pass the training dataset (X_train) to pca's 'fit()' method\n",
    "# pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"proportion de variance associée aux 10 premiéres axes:\\n\",pca.explained_variance_ratio_[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"proportion total de variance associées aux axes: \",round(sum(pca.explained_variance_ratio_),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply dimensionality reduction to X.\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_Logit = Logit_gscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('WADE_premier_test.txt', np.transpose(y_pred_Logit))\n",
    "# y_pred_Logit = Logit_gscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_dt = clf_DT.predict(X_test)\n",
    "# confusion_matrix(y_test, y_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construire la courbe ROC\n",
    "# from sklearn import metrics\n",
    "# fpr, tpr, thr = metrics.roc_curve(y_test, y_dt)\n",
    "\n",
    "# # calculer l'aire sous la courbe ROC\n",
    "# auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# # créer une figure\n",
    "# from matplotlib import pyplot as plt\n",
    "# fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# # afficher la courbe ROC\n",
    "# plt.plot(fpr, tpr, '-', lw=2, label='AUC=%.2f' % auc)\n",
    "\n",
    "# # donner un titre aux axes et au graphique\n",
    "# plt.xlabel('False Positive Rate', fontsize=16)\n",
    "# plt.ylabel('True Positive Rate', fontsize=16)\n",
    "# plt.title('Courbe ROC Décision Tree', fontsize=16)\n",
    "\n",
    "# # afficher la légende\n",
    "# plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "# # afficher l'image\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Score_test = {}\n",
    "# Score_train = {}\n",
    "\n",
    "\n",
    "# for variance in range(20):\n",
    "#     print(\"\\nVariance\",variance)\n",
    "    \n",
    "#     Col_train = Xtrainchallenge.std() > variance\n",
    "#     Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "#     Col_test = Xtestchallenge.std() > variance\n",
    "#     Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "#     Col = list(set(Col_train) & set(Col_test))\n",
    "    \n",
    "#     Xtrainchallenge = Xtrainchallenge[Col]\n",
    "#     Xtestchallenge = Xtestchallenge[Col]\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.1)\n",
    "    \n",
    "#     parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "#     clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "#     clf_DT.fit(X_train, y_train)\n",
    "    \n",
    "#     print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "#     print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "    \n",
    "#     Score_test[variance] = round(clf_DT.score(X_test, y_test),2)\n",
    "#     Score_train[variance] = round(clf_DT.score(X_train, y_train),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# variance = 1\n",
    "\n",
    "# Col_train = Xtrainchallenge.std() > variance\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Col_test = Xtestchallenge.std() > variance\n",
    "# Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "# Col = list(set(Col_train) & set(Col_test))\n",
    "    \n",
    "# Xtrainchallenge = Xtrainchallenge[Col]\n",
    "# Xtestchallenge = Xtestchallenge[Col]\n",
    "    \n",
    "    \n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "# }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(Xtrainchallenge, Ytrainchallenge)\n",
    "    \n",
    "# print(\"Train: \",clf_DT.score(Xtrainchallenge, Ytrainchallenge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf_DT .predict(Xtestchallenge)\n",
    "# np.savetxt('WADE_ArbreDecision.txt', np.transpose(y_pred),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "variance = 10\n",
    "\n",
    "Col_train = Xtrainchallenge.std() > variance\n",
    "Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "Col_test = Xtestchallenge.std() > variance\n",
    "Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "Col = list(set(Col_train) & set(Col_test))\n",
    "\n",
    "Xtrainchallenge = Xtrainchallenge[Col]\n",
    "Xtestchallenge = Xtestchallenge[Col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\", \"exponential\"],\n",
    "#     \"learning_rate\": [0.001, 0.005, 0.01],\n",
    "#     \"min_samples_leaf\": [0.0001, 0.005, 0.01],\n",
    "#      \"min_samples_split\" : [2, 10],\n",
    "#      \"min_impurity_decrease\" : [0.0, 0.001, 0.005], \n",
    "#     \"max_depth\":[8, 10],\n",
    "#     \"max_features\":[\"sqrt\", 0.5, 0.75],\n",
    "#     \"criterion\": [\"friedman_mse\"],\n",
    "#     \"subsample\":[0.5, 1],\n",
    "#     \"n_estimators\":[200, 400, 500],\n",
    "#     }\n",
    "        \n",
    "# clf_GB = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_GB.fit(Xtrainchallenge, Ytrainchallenge)\n",
    "\n",
    "# print(\"Train: \",clf_GB.score(Xtrainchallenge, Ytrainchallenge))\n",
    "# print(\"\\n\")\n",
    "# print(clf_GB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # y_pred = clf_GB.predict(Xtestchallenge)\n",
    "# np.savetxt('WADE_GradientBoosting_Version_2.txt', np.transpose(y_pred),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'modeles/clf_GB_2'\n",
    "# joblib.dump(clf_GB, filename)\n",
    "\n",
    "# # # load the model from disk\n",
    "# # loaded_model = joblib.load('modeles/clf_GB_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "#pour les scores\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "#ROC Curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #instantiate model and train\n",
    "\n",
    "# n_estimators = [100, 200, 300, 400, 500]\n",
    "# learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "# myXGBoost = XGBClassifier(learning_rate = 0.005, n_estimators=400, max_depth=8)\n",
    "# eval_set = [(X_test, y_test)]\n",
    "# eval_metric = [\"auc\",\"error\",\"logloss\"]\n",
    "# myXGBoost.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"error\", eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions for test set\n",
    "# y_pred = myXGBoost.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5832 candidates, totalling 29160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    4.8s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   12.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   17.1s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   22.0s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   28.3s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   34.9s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:   42.0s\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:   49.9s\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:   57.8s\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  2.5min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed:  3.3min\n",
      "[Parallel(n_jobs=-1)]: Done 330 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 384 tasks      | elapsed:  4.4min\n",
      "[Parallel(n_jobs=-1)]: Done 413 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 442 tasks      | elapsed:  5.2min\n",
      "[Parallel(n_jobs=-1)]: Done 473 tasks      | elapsed:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done 504 tasks      | elapsed:  6.0min\n",
      "[Parallel(n_jobs=-1)]: Done 537 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 570 tasks      | elapsed:  7.2min\n",
      "[Parallel(n_jobs=-1)]: Done 605 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed:  9.4min\n",
      "[Parallel(n_jobs=-1)]: Done 677 tasks      | elapsed: 10.6min\n",
      "[Parallel(n_jobs=-1)]: Done 714 tasks      | elapsed: 11.6min\n",
      "[Parallel(n_jobs=-1)]: Done 753 tasks      | elapsed: 12.7min\n",
      "[Parallel(n_jobs=-1)]: Done 792 tasks      | elapsed: 13.8min\n",
      "[Parallel(n_jobs=-1)]: Done 833 tasks      | elapsed: 15.2min\n",
      "[Parallel(n_jobs=-1)]: Done 874 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 917 tasks      | elapsed: 17.7min\n",
      "[Parallel(n_jobs=-1)]: Done 960 tasks      | elapsed: 18.9min\n",
      "[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 20.3min\n",
      "[Parallel(n_jobs=-1)]: Done 1050 tasks      | elapsed: 22.0min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-7e047b1c92a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0meval_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrainchallenge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrainchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mclf_XGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrainchallenge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrainchallenge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auc\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train: \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf_XGB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtrainchallenge\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mYtrainchallenge\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    665\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 667\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    932\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 934\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    935\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    936\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    831\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    425\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"eta\" : [0.1, 0.3, 0.6],\n",
    "    \"gamma\" : [0.1, 0.66],\n",
    "    \"subsample\": [0.5, 0.75, 1.0],\n",
    "    \"colsample_bytree\": [0.5, 0.75, 1.0],\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01],\n",
    "    \"min_samples_leaf\": [0.0005, 0.05],\n",
    "    \"max_depth\":[5, 8],\n",
    "    \"max_features\":[0.5, 0.75, 1.0],\n",
    "    \"n_estimators\" : [200, 400, 600],\n",
    "    }\n",
    "\n",
    "                \n",
    "clf_XGB = GridSearchCV(XGBClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "eval_set = [(Xtrainchallenge, Ytrainchallenge)]\n",
    "                    \n",
    "clf_XGB.fit(Xtrainchallenge, Ytrainchallenge, early_stopping_rounds=25, eval_metric=\"auc\", eval_set=eval_set, verbose=True)\n",
    "\n",
    "print(\"Train: \",clf_XGB.score(Xtrainchallenge, Ytrainchallenge))\n",
    "print(\"\\n\")\n",
    "print(clf_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'modeles/clf_XGBOOST_2'\n",
    "joblib.dump(clf_XGB, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_XGB = clf_XGB.predict(Xtestchallenge)\n",
    "np.savetxt('WADE_XGBoost_Version_2.txt', np.transpose(y_pred_XGB),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"loss\":[\"deviance\", \"exponential\"],\n",
    "    \"learning_rate\": [0.001, 0.005, 0.01],\n",
    "    \"min_samples_leaf\": [0.0001, 0.005, 0.01],\n",
    "     \"min_samples_split\" : [2, 10],\n",
    "     \"min_impurity_decrease\" : [0.0, 0.001, 0.005], \n",
    "    \"max_depth\":[8, 10],\n",
    "    \"max_features\":[\"sqrt\", 0.5, 0.75],\n",
    "    \"criterion\": [\"friedman_mse\"],\n",
    "    \"subsample\":[0.5, 1],\n",
    "    \"n_estimators\":[200, 400, 500],\n",
    "    }\n",
    "        \n",
    "clf_GB = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "clf_GB.fit(Xtrainchallenge, Ytrainchallenge)\n",
    "\n",
    "print(\"Train: \",clf_GB.score(Xtrainchallenge, Ytrainchallenge))\n",
    "print(\"\\n\")\n",
    "print(clf_GB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf_GB.predict(Xtestchallenge)\n",
    "np.savetxt('WADE_GradientBoosting_Version_2.txt', np.transpose(y_pred),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'modeles/clf_GB_2'\n",
    "joblib.dump(clf_GB, filename)\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = joblib.load('modeles/clf_GB_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyperopt est une bibliothèque python pour optimiser les espaces de recherche. Actuellement, il offre deux algorithmes d'optimisation : 1. Recherche aléatoire et 2. Tree of Parzen Estimators (TPE) qui est une approche bayésienne qui utilise P(x|y) au lieu de P(y|x), basée sur l'approximation de deux distributions différentes séparées par un seuil au lieu d'une pour calculer l'amélioration attendue (voir ceci). Auparavant, il s'adaptait aux processus gaussiens et aux arbres de régression, mais aujourd'hui, ils ne sont plus mis en œuvre."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fmin() est la fonction principale d'hyperopt pour l'optimisation. Il accepte quatre arguments de base et sort le jeu de paramètres optimisé :\n",
    "\n",
    "1. Objectif Fonction - fn\n",
    "2. Espace de recherche - espace\n",
    "3. Algorithme de recherche - algo\n",
    "4. (Maximum) nombre d'évaluations - max_evals\n",
    "\n",
    "Nous pouvons également passer un objet Trials à l'argument trials qui garde la trace de tout le processus. Pour fonctionner avec des tracés, la sortie de la fonction objectif doit être un dictionnaire comprenant au moins les clés \"perte\" et \"état\" qui contiennent respectivement le résultat et l'état d'optimisation. Les valeurs intermédiaires pourraient être extraites comme suit :\n",
    "\n",
    "1. trials.trials - une liste de dictionnaires contient toutes les informations pertinentes\n",
    "2. trials.results - une liste de dictionnaires collectant les sorties des fonctions\n",
    "3. trials.losses() - une liste des pertes (flottant pour chaque essai'ok')\n",
    "4. trials.statuses() - une liste de chaînes de statuts\n",
    "5. trials.vals - un dictionnaire des paramètres échantillonnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(params):\n",
    "#     num_round = int(params['n_estimators'])\n",
    "#     del params['n_estimators']\n",
    "#     dtrain = xgboost.DMatrix(Xtrainchallenge, label=y_train)\n",
    "#     dvalid = xgboost.DMatrix(Xtestchallenge, label=y_val)\n",
    "#     watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "#     gbm_model = xgboost.train(params, \n",
    "#                               dtrain, \n",
    "#                               num_round,\n",
    "#                               evals=watchlist,\n",
    "#                               verbose_eval=False)\n",
    "#     predictions = gbm_model.predict(dvalid, ntree_limit=gbm_model.best_iteration)\n",
    "#     print(gini_normalized(y_val, np.array(predictions)))\n",
    "#     loss = 1 - gini_normalized(y_val, np.array(predictions))\n",
    "#     return {'loss': loss, 'status': STATUS_OK}\n",
    " \n",
    " \n",
    "# def optimize(evals, cores, trials, optimizer=tpe.suggest, random_state=0):\n",
    "#     space = {\n",
    "#         'n_estimators': hp.quniform('n_estimators', 200, 600, 1),\n",
    "#         'eta': hp.quniform('eta', 0.025, 0.25, 0.025), # A problem with max_depth casted to float instead of int with the hp.quniform method.\n",
    "#         'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "#         'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "#         'subsample': hp.quniform('subsample', 0.7, 1, 0.05),\n",
    "#         'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "#         'colsample_bytree': hp.quniform('colsample_bytree', 0.7, 1, 0.05),\n",
    "#         'alpha' :  hp.quniform('alpha', 0, 10, 1),\n",
    "#         'lambda': hp.quniform('lambda', 1, 2, 0.1),\n",
    "#         'nthread': cores,\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'booster': 'gbtree',\n",
    "#         'seed': random_state\n",
    "#     }\n",
    "#     best = fmin(score, space, algo=tpe.suggest, max_evals=evals, trials = trials)\n",
    "#     return best\n",
    "    \n",
    "\n",
    "# trials = Trials()\n",
    "# cores = 32\n",
    "# n= 1000\n",
    "# start = time.time()\n",
    "# best_param = optimize(evals = n,\n",
    "#                       optimizer=tpe.suggest,\n",
    "#                       cores = cores,\n",
    "#                       trials = trials)\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"The best hyperparameters are: \", \"\\n\")\n",
    "# print(best_param)\n",
    "# end = time.time()\n",
    "# print('Time elapsed to optimize {0} executions: {1}'.format(n,end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Avec un arbre de décision pour une variance de 1, on obtient un score de 73.8%\n",
    "Avec un Gradient Boosting pour une variance de 9, on obtient un score de 77.4%\n",
    "Avec XGBoost pour une variance de 9, on obtient un score de 78.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
