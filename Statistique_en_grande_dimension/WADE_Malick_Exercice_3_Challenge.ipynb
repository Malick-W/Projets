{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Communities and Crime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WADE El Hadji Malick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt    \n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#pour les scores\n",
    "from sklearn.metrics import f1_score\n",
    "#from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import accuracy_score\n",
    "#ROC Curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "\n",
    "from sklearn.externals import joblib # save and load models\n",
    "\n",
    "# # save the model to disk\n",
    "# filename = 'modeles/SVM'\n",
    "# joblib.dump(SVM, filename)\n",
    "\n",
    "# # load the model from disk\n",
    "# loaded_model = joblib.load(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Nettoyage des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "path_Home = \"/home/jovyan/work/Projets/Projets/Statistique_en_grande_dimension/donnees_challenge\"\n",
    "#path = \"/users/mmath/wade/Bureau/Data/Statistique_en_grande_dimension\"\n",
    "\n",
    "Xtrainchallenge = pd.read_csv(path_Home + \"/Xtrainchallenge.txt\",  sep=' ')\n",
    "Ytrainchallenge = pd.read_csv(path_Home + \"/Ytrainchallenge.txt\",  sep=' ')\n",
    "\n",
    "Xtestchallenge = pd.read_csv(path_Home + \"/Xtestchallenge.txt\",  sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lignes:  1000\n",
      "Colonnes:  396\n",
      "---------------\n",
      "\n",
      "Variables:\n"
     ]
    }
   ],
   "source": [
    "print (\"Lignes: \" ,Xtrainchallenge.shape[0])\n",
    "print (\"Colonnes: \" ,Xtrainchallenge.shape[1])\n",
    "\n",
    "print(\"---------------\")\n",
    "print (\"\\nVariables:\")\n",
    "# print(Xtrainchallenge.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Col_train = Xtrainchallenge.std() > 1.5\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Xtrainchallenge = Xtrainchallenge[Col_train]\n",
    "# Xtestchallenge = Xtestchallenge[Col_train]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.3)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xtrainchallenge = Xtrainchallenge[Col_train]\n",
    "# Xtestchallenge = Xtestchallenge[Col_train]\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.3)\n",
    "\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # On regarde la matrice de corrélation\n",
    "# fig = plt.figure(1, figsize=(50, 50))\n",
    "\n",
    "# # sns.heatmap(round(Xtrainchallenge.corr(),2), cmap=sns.diverging_palette(20, 220, n=200), fmt=\".2f\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print (\"Lignes: \" ,Xtrainchallenge.shape[0])\n",
    "# print (\"Colonnes: \" ,Xtrainchallenge.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Modèles de prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# start = time.time()\n",
    "\n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(X_train, y_train)\n",
    "\n",
    "# end=time.time()\n",
    "# train_time_dec=end-start\n",
    "\n",
    "# print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "# print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "# print(\"\\n\")\n",
    "# print(clf_DT.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = Xtrainchallenge\n",
    "# X_test = Xtestchallenge\n",
    "# y_train = Ytrainchallenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_components = 150\n",
    "\n",
    "# # TODO: Create an instance of PCA, initializing with n_components=n_components and whiten=True\n",
    "# pca = PCA(n_components=n_components, whiten=True, svd_solver='randomized')\n",
    "\n",
    "# #TODO: pass the training dataset (X_train) to pca's 'fit()' method\n",
    "# pca = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"proportion de variance associée aux 10 premiéres axes:\\n\",pca.explained_variance_ratio_[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"proportion total de variance associées aux axes: \",round(sum(pca.explained_variance_ratio_),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply dimensionality reduction to X.\n",
    "# X_train_pca = pca.transform(X_train)\n",
    "# X_test_pca = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_Logit = Logit_gscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.savetxt('WADE_premier_test.txt', np.transpose(y_pred_Logit))\n",
    "# y_pred_Logit = Logit_gscv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Informations sur les données de Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance = 10\n",
    "\n",
    "# Col_train = Xtrainchallenge.std() > variance\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Col_test = Xtestchallenge.std() > variance\n",
    "# Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "# Col = list(set(Col_train) & set(Col_test))\n",
    "    \n",
    "# Xtrainchallenge = Xtrainchallenge[Col]\n",
    "# Xtestchallenge = Xtestchallenge[Col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Precision_test = {}\n",
    "Precision_train = {}\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "parameters = {\n",
    "    \"criterion\": [\"gini\", \"entropy\"],\n",
    "    \"max_depth\":[3, 5, 8, 10],\n",
    "    \"min_samples_split\": [2, 0.001, 0.005],\n",
    "    \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "    \"max_features\": [1., \"sqrt\"]\n",
    "    }\n",
    "\n",
    "clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "for size in []:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = size)\n",
    "    \n",
    "    clf_DT.fit(X_train, y_train)\n",
    "    \n",
    "    print(size)\n",
    "    print(\"\\n\")\n",
    "    Precision_train[size] = clf_DT.score(X_train, y_train)\n",
    "    Precision_test[size]  = clf_DT.score(X_test, y_test)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_dt = clf_DT.predict(X_test)\n",
    "# confusion_matrix(y_test, y_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # construire la courbe ROC\n",
    "# from sklearn import metrics\n",
    "# fpr, tpr, thr = metrics.roc_curve(y_test, y_dt)\n",
    "\n",
    "# # calculer l'aire sous la courbe ROC\n",
    "# auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# # créer une figure\n",
    "# from matplotlib import pyplot as plt\n",
    "# fig = plt.figure(figsize=(7,7))\n",
    "\n",
    "# # afficher la courbe ROC\n",
    "# plt.plot(fpr, tpr, '-', lw=2, label='AUC=%.2f' % auc)\n",
    "\n",
    "# # donner un titre aux axes et au graphique\n",
    "# plt.xlabel('False Positive Rate', fontsize=16)\n",
    "# plt.ylabel('True Positive Rate', fontsize=16)\n",
    "# plt.title('Courbe ROC Décision Tree', fontsize=16)\n",
    "\n",
    "# # afficher la légende\n",
    "# plt.legend(loc=\"lower right\", fontsize=14)\n",
    "\n",
    "# # afficher l'image\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doonees_Test = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Random Forest', 'Gradient Boosting','XGBoost'],\n",
    "    'Accuracy_score': [as_dt, as_rf, as_grad, as_xgrad],\n",
    "    'Matrice de confusion': [cm_dt, cm_rf, cm_grad, cm_xgrad],\n",
    "    \"F1 [macro,micro, weighted]\": [f1_dt, f1_rf, f1_grad, f1_xgrad]})\n",
    "    \n",
    "models_cross.sort_values(by='Accuracy_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Score_test = {}\n",
    "# Score_train = {}\n",
    "\n",
    "\n",
    "# for variance in range(20):\n",
    "#     print(\"\\nVariance\",variance)\n",
    "    \n",
    "#     Col_train = Xtrainchallenge.std() > variance\n",
    "#     Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "#     Col_test = Xtestchallenge.std() > variance\n",
    "#     Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "#     Col = list(set(Col_train) & set(Col_test))\n",
    "    \n",
    "#     Xtrainchallenge = Xtrainchallenge[Col]\n",
    "#     Xtestchallenge = Xtestchallenge[Col]\n",
    "    \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(Xtrainchallenge,Ytrainchallenge,test_size = 0.1)\n",
    "    \n",
    "#     parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "#     }\n",
    "\n",
    "#     clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=1)\n",
    "\n",
    "#     clf_DT.fit(X_train, y_train)\n",
    "    \n",
    "#     print(\"Train: \",clf_DT.score(X_train, y_train))\n",
    "#     print(\"Test: \",clf_DT.score(X_test, y_test))\n",
    "    \n",
    "#     Score_test[variance] = round(clf_DT.score(X_test, y_test),2)\n",
    "#     Score_train[variance] = round(clf_DT.score(X_train, y_train),2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# variance = 1\n",
    "\n",
    "# Col_train = Xtrainchallenge.std() > variance\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Col_test = Xtestchallenge.std() > variance\n",
    "# Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "# Col = list(set(Col_train) & set(Col_test))\n",
    "    \n",
    "# Xtrainchallenge = Xtrainchallenge[Col]\n",
    "# Xtestchallenge = Xtestchallenge[Col]\n",
    "    \n",
    "    \n",
    "# parameters = {\n",
    "#     \"criterion\": [\"gini\", \"entropy\"],\n",
    "#     \"max_depth\":[3, 5, 8, 10],\n",
    "#     \"min_samples_split\": [2, 0.001, 0.005],\n",
    "#     \"min_samples_leaf\": [0.001, 0.005, 0.01, 0.01, 0.05],\n",
    "#     \"max_features\": [1., \"sqrt\"]\n",
    "# }\n",
    "\n",
    "# clf_DT = GridSearchCV(DecisionTreeClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_DT.fit(Xtrainchallenge, Ytrainchallenge)\n",
    "    \n",
    "# print(\"Train: \",clf_DT.score(Xtrainchallenge, Ytrainchallenge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf_DT .predict(Xtestchallenge)\n",
    "# np.savetxt('WADE_ArbreDecision.txt', np.transpose(y_pred),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance = 10\n",
    "\n",
    "# Col_train = Xtrainchallenge.std() > variance\n",
    "# Col_train = [i for i in Col_train.index if Col_train[i]==True]\n",
    "\n",
    "# Col_test = Xtestchallenge.std() > variance\n",
    "# Col_test = [i for i in Col_test.index if Col_test[i]==True]\n",
    "\n",
    "# Col = list(set(Col_train) & set(Col_test))\n",
    "\n",
    "# Xtrainchallenge = Xtrainchallenge[Col]\n",
    "# Xtestchallenge = Xtestchallenge[Col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# #pour les scores\n",
    "# from sklearn.metrics import f1_score\n",
    "# #from sklearn.metrics import matthews_corrcoef\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# #ROC Curve\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# #instantiate model and train\n",
    "\n",
    "# n_estimators = [100, 200, 300, 400, 500]\n",
    "# learning_rate = [0.0001, 0.001, 0.01, 0.1]\n",
    "# myXGBoost = XGBClassifier(learning_rate = 0.005, n_estimators=400, max_depth=8)\n",
    "# eval_set = [(X_test, y_test)]\n",
    "# eval_metric = [\"auc\",\"error\",\"logloss\"]\n",
    "# myXGBoost.fit(X_train, y_train, early_stopping_rounds=30, eval_metric=\"error\", eval_set=eval_set, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make predictions for test set\n",
    "# y_pred = myXGBoost.predict(X_test)\n",
    "\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy: %.2f%%\" % (accuracy * 100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 64 candidates, totalling 320 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   5 tasks      | elapsed:    5.9s\n",
      "[Parallel(n_jobs=-1)]: Done  10 tasks      | elapsed:    8.7s\n",
      "[Parallel(n_jobs=-1)]: Done  17 tasks      | elapsed:   14.1s\n",
      "[Parallel(n_jobs=-1)]: Done  24 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:   38.9s\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:   51.9s\n",
      "[Parallel(n_jobs=-1)]: Done  53 tasks      | elapsed:   59.7s\n",
      "[Parallel(n_jobs=-1)]: Done  64 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done  77 tasks      | elapsed:  1.7min\n",
      "[Parallel(n_jobs=-1)]: Done  90 tasks      | elapsed:  1.9min\n",
      "[Parallel(n_jobs=-1)]: Done 105 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=-1)]: Done 120 tasks      | elapsed:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done 137 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed:  3.5min\n",
      "[Parallel(n_jobs=-1)]: Done 173 tasks      | elapsed:  4.1min\n",
      "[Parallel(n_jobs=-1)]: Done 192 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 213 tasks      | elapsed:  6.5min\n",
      "[Parallel(n_jobs=-1)]: Done 234 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 257 tasks      | elapsed:  8.9min\n",
      "[Parallel(n_jobs=-1)]: Done 280 tasks      | elapsed: 10.8min\n",
      "[Parallel(n_jobs=-1)]: Done 305 tasks      | elapsed: 12.0min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-auc:0.971598\n",
      "Will train until validation_0-auc hasn't improved in 25 rounds.\n",
      "[1]\tvalidation_0-auc:0.98966\n",
      "[2]\tvalidation_0-auc:0.992014\n",
      "[3]\tvalidation_0-auc:0.992878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 320 out of 320 | elapsed: 13.3min finished\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\tvalidation_0-auc:0.996712\n",
      "[5]\tvalidation_0-auc:0.996794\n",
      "[6]\tvalidation_0-auc:0.99663\n",
      "[7]\tvalidation_0-auc:0.997796\n",
      "[8]\tvalidation_0-auc:0.998152\n",
      "[9]\tvalidation_0-auc:0.99836\n",
      "[10]\tvalidation_0-auc:0.99862\n",
      "[11]\tvalidation_0-auc:0.998444\n",
      "[12]\tvalidation_0-auc:0.998712\n",
      "[13]\tvalidation_0-auc:0.998824\n",
      "[14]\tvalidation_0-auc:0.998812\n",
      "[15]\tvalidation_0-auc:0.998832\n",
      "[16]\tvalidation_0-auc:0.998944\n",
      "[17]\tvalidation_0-auc:0.99902\n",
      "[18]\tvalidation_0-auc:0.999024\n",
      "[19]\tvalidation_0-auc:0.999128\n",
      "[20]\tvalidation_0-auc:0.99922\n",
      "[21]\tvalidation_0-auc:0.9992\n",
      "[22]\tvalidation_0-auc:0.999252\n",
      "[23]\tvalidation_0-auc:0.99932\n",
      "[24]\tvalidation_0-auc:0.99936\n",
      "[25]\tvalidation_0-auc:0.999452\n",
      "[26]\tvalidation_0-auc:0.999424\n",
      "[27]\tvalidation_0-auc:0.999492\n",
      "[28]\tvalidation_0-auc:0.9995\n",
      "[29]\tvalidation_0-auc:0.99956\n",
      "[30]\tvalidation_0-auc:0.999608\n",
      "[31]\tvalidation_0-auc:0.999648\n",
      "[32]\tvalidation_0-auc:0.99962\n",
      "[33]\tvalidation_0-auc:0.999632\n",
      "[34]\tvalidation_0-auc:0.99964\n",
      "[35]\tvalidation_0-auc:0.999648\n",
      "[36]\tvalidation_0-auc:0.999676\n",
      "[37]\tvalidation_0-auc:0.999676\n",
      "[38]\tvalidation_0-auc:0.999668\n",
      "[39]\tvalidation_0-auc:0.999708\n",
      "[40]\tvalidation_0-auc:0.99972\n",
      "[41]\tvalidation_0-auc:0.999752\n",
      "[42]\tvalidation_0-auc:0.999812\n",
      "[43]\tvalidation_0-auc:0.999832\n",
      "[44]\tvalidation_0-auc:0.999852\n",
      "[45]\tvalidation_0-auc:0.999864\n",
      "[46]\tvalidation_0-auc:0.999872\n",
      "[47]\tvalidation_0-auc:0.99988\n",
      "[48]\tvalidation_0-auc:0.999876\n",
      "[49]\tvalidation_0-auc:0.999876\n",
      "[50]\tvalidation_0-auc:0.999888\n",
      "[51]\tvalidation_0-auc:0.999892\n",
      "[52]\tvalidation_0-auc:0.999888\n",
      "[53]\tvalidation_0-auc:0.999892\n",
      "[54]\tvalidation_0-auc:0.999896\n",
      "[55]\tvalidation_0-auc:0.9999\n",
      "[56]\tvalidation_0-auc:0.9999\n",
      "[57]\tvalidation_0-auc:0.9999\n",
      "[58]\tvalidation_0-auc:0.999912\n",
      "[59]\tvalidation_0-auc:0.999912\n",
      "[60]\tvalidation_0-auc:0.999912\n",
      "[61]\tvalidation_0-auc:0.99992\n",
      "[62]\tvalidation_0-auc:0.999916\n",
      "[63]\tvalidation_0-auc:0.999924\n",
      "[64]\tvalidation_0-auc:0.99992\n",
      "[65]\tvalidation_0-auc:0.999924\n",
      "[66]\tvalidation_0-auc:0.999928\n",
      "[67]\tvalidation_0-auc:0.999932\n",
      "[68]\tvalidation_0-auc:0.999928\n",
      "[69]\tvalidation_0-auc:0.999936\n",
      "[70]\tvalidation_0-auc:0.999936\n",
      "[71]\tvalidation_0-auc:0.999924\n",
      "[72]\tvalidation_0-auc:0.999936\n",
      "[73]\tvalidation_0-auc:0.999944\n",
      "[74]\tvalidation_0-auc:0.999952\n",
      "[75]\tvalidation_0-auc:0.999964\n",
      "[76]\tvalidation_0-auc:0.99996\n",
      "[77]\tvalidation_0-auc:0.999964\n",
      "[78]\tvalidation_0-auc:0.99996\n",
      "[79]\tvalidation_0-auc:0.999968\n",
      "[80]\tvalidation_0-auc:0.999968\n",
      "[81]\tvalidation_0-auc:0.999968\n",
      "[82]\tvalidation_0-auc:0.999968\n",
      "[83]\tvalidation_0-auc:0.999968\n",
      "[84]\tvalidation_0-auc:0.999968\n",
      "[85]\tvalidation_0-auc:0.999968\n",
      "[86]\tvalidation_0-auc:0.999968\n",
      "[87]\tvalidation_0-auc:0.999968\n",
      "[88]\tvalidation_0-auc:0.999972\n",
      "[89]\tvalidation_0-auc:0.999976\n",
      "[90]\tvalidation_0-auc:0.99998\n",
      "[91]\tvalidation_0-auc:0.999984\n",
      "[92]\tvalidation_0-auc:0.999984\n",
      "[93]\tvalidation_0-auc:0.999984\n",
      "[94]\tvalidation_0-auc:0.999992\n",
      "[95]\tvalidation_0-auc:0.999992\n",
      "[96]\tvalidation_0-auc:0.999988\n",
      "[97]\tvalidation_0-auc:0.999988\n",
      "[98]\tvalidation_0-auc:0.999992\n",
      "[99]\tvalidation_0-auc:0.999992\n",
      "[100]\tvalidation_0-auc:0.999992\n",
      "[101]\tvalidation_0-auc:0.999992\n",
      "[102]\tvalidation_0-auc:0.999996\n",
      "[103]\tvalidation_0-auc:0.999996\n",
      "[104]\tvalidation_0-auc:0.999996\n",
      "[105]\tvalidation_0-auc:0.999996\n",
      "[106]\tvalidation_0-auc:1\n",
      "[107]\tvalidation_0-auc:1\n",
      "[108]\tvalidation_0-auc:1\n",
      "[109]\tvalidation_0-auc:1\n",
      "[110]\tvalidation_0-auc:1\n",
      "[111]\tvalidation_0-auc:1\n",
      "[112]\tvalidation_0-auc:1\n",
      "[113]\tvalidation_0-auc:1\n",
      "[114]\tvalidation_0-auc:1\n",
      "[115]\tvalidation_0-auc:1\n",
      "[116]\tvalidation_0-auc:1\n",
      "[117]\tvalidation_0-auc:1\n",
      "[118]\tvalidation_0-auc:1\n",
      "[119]\tvalidation_0-auc:1\n",
      "[120]\tvalidation_0-auc:1\n",
      "[121]\tvalidation_0-auc:1\n",
      "[122]\tvalidation_0-auc:1\n",
      "[123]\tvalidation_0-auc:1\n",
      "[124]\tvalidation_0-auc:1\n",
      "[125]\tvalidation_0-auc:1\n",
      "[126]\tvalidation_0-auc:1\n",
      "[127]\tvalidation_0-auc:1\n",
      "[128]\tvalidation_0-auc:1\n",
      "[129]\tvalidation_0-auc:1\n",
      "[130]\tvalidation_0-auc:1\n",
      "[131]\tvalidation_0-auc:1\n",
      "Stopping. Best iteration:\n",
      "[106]\tvalidation_0-auc:1\n",
      "\n",
      "Train:  0.999\n",
      "\n",
      "\n",
      "{'colsample_bytree': 0.5, 'eta': 0.1, 'gamma': 0.66, 'learning_rate': 0.01, 'max_depth': 8, 'max_features': 0.5, 'min_samples_leaf': 0.01, 'n_estimators': 500, 'subsample': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# parameters = {\n",
    "#     \"eta\" : [0.1, 0.6],\n",
    "#     \"gamma\" : [0.1, 0.66],\n",
    "#     \"subsample\": [0.5, 1.0],\n",
    "#     \"colsample_bytree\": [0.5, 1.0],\n",
    "#     \"learning_rate\": [0.001, 0.01],\n",
    "#     \"min_samples_leaf\": [0.01],\n",
    "#     \"max_depth\":[8],\n",
    "#     \"max_features\":[0.5, 1.0],\n",
    "#     \"n_estimators\" : [500],\n",
    "#     }\n",
    "\n",
    "                \n",
    "# clf_XGB = GridSearchCV(XGBClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# eval_set = [(Xtrainchallenge, Ytrainchallenge)]\n",
    "                    \n",
    "# clf_XGB.fit(Xtrainchallenge, Ytrainchallenge, early_stopping_rounds=25, eval_metric=\"auc\", eval_set=eval_set, verbose=True)\n",
    "\n",
    "# print(\"Train: \",clf_XGB.score(Xtrainchallenge, Ytrainchallenge))\n",
    "# print(\"\\n\")\n",
    "# print(clf_XGB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['modeles/clf_XGBOOST_2']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'modeles/clf_XGBOOST_2'\n",
    "# joblib.dump(clf_XGB, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred_XGB = clf_XGB.predict(Xtestchallenge)\n",
    "# np.savetxt('WADE_XGBoost_Version_2.txt', np.transpose(y_pred_XGB),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# parameters = {\n",
    "#     \"loss\":[\"deviance\", \"exponential\"],\n",
    "#     \"learning_rate\": [0.001, 0.01],\n",
    "#     \"min_samples_leaf\": [0.00010.01],\n",
    "#      \"min_samples_split\" : [2, 10],\n",
    "#      \"min_impurity_decrease\" : [0.00.005], \n",
    "#     \"max_depth\":[8, 10],\n",
    "#     \"max_features\":[\"sqrt\"],\n",
    "#     \"criterion\": [\"friedman_mse\"],\n",
    "#     \"subsample\":[0.5, 1],\n",
    "#     \"n_estimators\":[500],\n",
    "#     }\n",
    "        \n",
    "# clf_GB = GridSearchCV(GradientBoostingClassifier(), parameters, cv=5, n_jobs=-1,verbose=10)\n",
    "\n",
    "# clf_GB.fit(Xtrainchallenge, Ytrainchallenge)\n",
    "\n",
    "# print(\"Train: \",clf_GB.score(Xtrainchallenge, Ytrainchallenge))\n",
    "# print(\"\\n\")\n",
    "# print(clf_GB.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = clf_GB.predict(Xtestchallenge)\n",
    "# np.savetxt('WADE_GradientBoosting_Version_2.txt', np.transpose(y_pred),fmt='% 0d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model to disk\n",
    "# filename = 'modeles/clf_GB_2'\n",
    "# joblib.dump(clf_GB, filename)\n",
    "\n",
    "# # # load the model from disk\n",
    "# # loaded_model = joblib.load('modeles/clf_GB_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from hyperopt import hp, fmin, tpe, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hyperopt est une bibliothèque python pour optimiser les espaces de recherche. Actuellement, il offre deux algorithmes d'optimisation : 1. Recherche aléatoire et 2. Tree of Parzen Estimators (TPE) qui est une approche bayésienne qui utilise P(x|y) au lieu de P(y|x), basée sur l'approximation de deux distributions différentes séparées par un seuil au lieu d'une pour calculer l'amélioration attendue (voir ceci). Auparavant, il s'adaptait aux processus gaussiens et aux arbres de régression, mais aujourd'hui, ils ne sont plus mis en œuvre."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "fmin() est la fonction principale d'hyperopt pour l'optimisation. Il accepte quatre arguments de base et sort le jeu de paramètres optimisé :\n",
    "\n",
    "1. Objectif Fonction - fn\n",
    "2. Espace de recherche - espace\n",
    "3. Algorithme de recherche - algo\n",
    "4. (Maximum) nombre d'évaluations - max_evals\n",
    "\n",
    "Nous pouvons également passer un objet Trials à l'argument trials qui garde la trace de tout le processus. Pour fonctionner avec des tracés, la sortie de la fonction objectif doit être un dictionnaire comprenant au moins les clés \"perte\" et \"état\" qui contiennent respectivement le résultat et l'état d'optimisation. Les valeurs intermédiaires pourraient être extraites comme suit :\n",
    "\n",
    "1. trials.trials - une liste de dictionnaires contient toutes les informations pertinentes\n",
    "2. trials.results - une liste de dictionnaires collectant les sorties des fonctions\n",
    "3. trials.losses() - une liste des pertes (flottant pour chaque essai'ok')\n",
    "4. trials.statuses() - une liste de chaînes de statuts\n",
    "5. trials.vals - un dictionnaire des paramètres échantillonnés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(params):\n",
    "#     num_round = int(params['n_estimators'])\n",
    "#     del params['n_estimators']\n",
    "#     dtrain = xgboost.DMatrix(Xtrainchallenge, label=y_train)\n",
    "#     dvalid = xgboost.DMatrix(Xtestchallenge, label=y_val)\n",
    "#     watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "#     gbm_model = xgboost.train(params, \n",
    "#                               dtrain, \n",
    "#                               num_round,\n",
    "#                               evals=watchlist,\n",
    "#                               verbose_eval=False)\n",
    "#     predictions = gbm_model.predict(dvalid, ntree_limit=gbm_model.best_iteration)\n",
    "#     print(gini_normalized(y_val, np.array(predictions)))\n",
    "#     loss = 1 - gini_normalized(y_val, np.array(predictions))\n",
    "#     return {'loss': loss, 'status': STATUS_OK}\n",
    " \n",
    " \n",
    "# def optimize(evals, cores, trials, optimizer=tpe.suggest, random_state=0):\n",
    "#     space = {\n",
    "#         'n_estimators': hp.quniform('n_estimators', 200, 600, 1),\n",
    "#         'eta': hp.quniform('eta', 0.025, 0.25, 0.025), # A problem with max_depth casted to float instead of int with the hp.quniform method.\n",
    "#         'max_depth':  hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "#         'min_child_weight': hp.quniform('min_child_weight', 1, 10, 1),\n",
    "#         'subsample': hp.quniform('subsample', 0.7, 1, 0.05),\n",
    "#         'gamma': hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "#         'colsample_bytree': hp.quniform('colsample_bytree', 0.7, 1, 0.05),\n",
    "#         'alpha' :  hp.quniform('alpha', 0, 10, 1),\n",
    "#         'lambda': hp.quniform('lambda', 1, 2, 0.1),\n",
    "#         'nthread': cores,\n",
    "#         'objective': 'binary:logistic',\n",
    "#         'booster': 'gbtree',\n",
    "#         'seed': random_state\n",
    "#     }\n",
    "#     best = fmin(score, space, algo=tpe.suggest, max_evals=evals, trials = trials)\n",
    "#     return best\n",
    "    \n",
    "\n",
    "# trials = Trials()\n",
    "# cores = 32\n",
    "# n= 1000\n",
    "# start = time.time()\n",
    "# best_param = optimize(evals = n,\n",
    "#                       optimizer=tpe.suggest,\n",
    "#                       cores = cores,\n",
    "#                       trials = trials)\n",
    "# print(\"------------------------------------\")\n",
    "# print(\"The best hyperparameters are: \", \"\\n\")\n",
    "# print(best_param)\n",
    "# end = time.time()\n",
    "# print('Time elapsed to optimize {0} executions: {1}'.format(n,end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Avec un arbre de décision pour une variance de 1, on obtient un score de 73.8%\n",
    "Avec un Gradient Boosting pour une variance de 9, on obtient un score de 77.4%\n",
    "Avec XGBoost pour une variance de 9, on obtient un score de 78.4%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
